{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing Lab (CODE): CC AI Ep 8",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF_ESTSJT9IX",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Data Analysis and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ff6wDmJUeN",
        "colab_type": "text"
      },
      "source": [
        "Natural language processing is teaching an AI system to understand and produce language, by asking it to find and copy patterns in human behavior. NLP is a huge part of artificial intelligence. \n",
        "\n",
        "To build a natural language processing AI, we need to do four main things: 1) gather and clean the data, 2) set up the model, 3) train the model, and 4) make inferences.\n",
        "\n",
        "First, we need to import our John Green transcripts from Vlogbrothers videos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bxV3YjC7psj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.1\n",
        "\n",
        "from urllib.request import urlopen\n",
        "txt = urlopen(\"https://raw.githubusercontent.com/crash-course-ai/lab2-nlp/master/vlogbrothers.txt\").read().decode('ascii').split(\"\\n\")\n",
        "print(\"Our dataset contains {} vlogbrothers scripts\".format(len(txt)))\n",
        "# ADVANCED_CHANGEME -- You can change this to load any text file \n",
        "# You want it to be one line of plain text for every script.  Extra\n",
        "# annotations like [John:] or *starts coughing* make learning more difficult.\n",
        "everything = set([w for s in txt for w in s.split()])\n",
        "print(\"and {} lexical types\".format(len(everything)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc3u5U44A8ap",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to do some preprocessing on our data to prepare it for our model. We're going to tokenize our text and split every sentence into a list of lexical tokens, including some rules for puctuation. And we're going to add markers for the start and end of each segment.\n",
        "\n",
        "We also want to divide all our data into training and validation datasets, so our model can learn from the training data, but we can test it on the validation dataset that it has never seen before. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6RgRCeEATzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.2\n",
        "\n",
        "# 1. Import the tokenizer\n",
        "import spacy\n",
        "nlp = spacy.load(\"en\", disable=[\"parser\",\"tagger\",\"ner\",\"textcat\"])\n",
        "\n",
        "# 2. Tokenize\n",
        "txt = [nlp(s) for s in txt]\n",
        "\n",
        "# 3. Mark the beginning and end of each script \n",
        "txt = [ [\"<s>\"] + [str(w) for w in s] + [\"</s>\"] for s in txt]\n",
        "\n",
        "# 4. Separate the data into training and validation\n",
        "train = txt[:-5]\n",
        "valid = txt[-5:]\n",
        "\n",
        "# 5. Flatten the lists into one long string and remove extra whitespace\n",
        "train = [w for s in train for w in s if not w.isspace()]\n",
        "valid = [w for s in valid for w in s if not w.isspace()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiFC8YqoC5jQ",
        "colab_type": "text"
      },
      "source": [
        "Great!  Now let's take a look at our data statistics -- specifically, how many lexical types and how many lexical tokens we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzYMScnsDQ2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.3\n",
        "\n",
        "\"\"\"\n",
        "    How big is our dataset?\n",
        "\"\"\"\n",
        "print(\"Our training dataset contains {} lexical types\".format(len(set(train))))\n",
        "print(\"Our training dataset contains {} lexical tokens\".format(len(train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09KAnNLIOaeE",
        "colab_type": "text"
      },
      "source": [
        "Let's take a closer look at the vocabulary and see what kinds of words human John Green likes to use more often than others. We’re going to figure out how many lexical types occur more than once, twice, and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avE76fKTOdDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.4\n",
        "\n",
        "# 1. Count the frequencies of every word\n",
        "from collections import Counter, defaultdict\n",
        "counts = Counter(train)\n",
        "\n",
        "frequencies = [0]*8\n",
        "for w in counts:\n",
        "  if counts[w] >= 128:\n",
        "    frequencies[0] += 1\n",
        "  elif counts[w] >= 64:\n",
        "    frequencies[1] += 1\n",
        "  elif counts[w] >= 32:\n",
        "    frequencies[2] += 1\n",
        "  elif counts[w] >= 16:\n",
        "    frequencies[3] += 1\n",
        "  elif counts[w] >= 8:\n",
        "    frequencies[4] += 1\n",
        "  elif counts[w] >= 4:\n",
        "    frequencies[5] += 1\n",
        "  elif counts[w] >= 2:\n",
        "    frequencies[6] += 1\n",
        "  else:\n",
        "    frequencies[7] += 1\n",
        "\n",
        "\n",
        "# 2. Plot their distributions\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "f,a = plt.subplots(1,1,figsize=(10,5))\n",
        "a.set(xlabel='Lexical types occuring more then n times', \n",
        "      ylabel='Number of lexical types')\n",
        "\n",
        "labels = [128, 64, 32, 16, 8, 4, 2, 1]\n",
        "_ = sns.barplot(labels, frequencies, ax=a, order=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2wDniV_sEo",
        "colab_type": "text"
      },
      "source": [
        "We have a lot of very rare words, words that only occur a single time in the dataset. These are very difficult to use when building a NLP model because our model will try to find and copy patterns, so it needs plenty of examples of how to use each word during training. Let's look at some of these rare words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHsXx2Kj_0AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.5\n",
        "\n",
        "from textwrap import wrap\n",
        "rare = [w for w in counts if counts[w] == 1]\n",
        "for line in wrap(\"   \".join([\"{:15s}\".format(w) for w in rare[:100]]), width=70):\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXjNAjt6QElF",
        "colab_type": "text"
      },
      "source": [
        "Some of these seem too difficult for our model to learn, like Liliputians. But the model could learn some of these if we helped simplify the words.\n",
        "\n",
        "For example, what if we got rid of numbers by replacing them with #s? And what if we use our knowledge of word morphology to remove some endings like -ed or -ing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H0t1qQ0AQhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.6\n",
        "\n",
        "# This is a little function to help us clean up the data\n",
        "# CHANGEME -- Introduce or remove rules\n",
        "import re\n",
        "def simplify(w):\n",
        "    # Remove extra punctuation\n",
        "    w = w.replace(\"-\", \"\").replace(\"~\",\"\")\n",
        "    \n",
        "    # Replace numbers with # sign\n",
        "    w = re.sub('\\d', '#', w)\n",
        "    \n",
        "    # Change some endings\n",
        "    if len(w) > 3 and w[-2:] in set([\"ed\", \"er\",\"ly\"]):\n",
        "        return [w[:-2], w[-2:]]\n",
        "    elif len(w) > 4 and w[-3:] in set([\"ing\",\"'re\"]):\n",
        "        return [w[:-3], w[-3:]]\n",
        "    return [w]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp6CiTzOQnua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.7\n",
        "\n",
        "# 1. Go through and clean all of our data\n",
        "train_clean = []\n",
        "for w in train:\n",
        "    for piece in simplify(w):\n",
        "        train_clean.append(piece)\n",
        "valid_clean = []\n",
        "for w in valid:\n",
        "    for piece in simplify(w):\n",
        "        valid_clean.append(piece)\n",
        "        \n",
        "\"\"\"\n",
        "    How big is our dataset?\n",
        "\"\"\"\n",
        "print(\"{} lexical types\".format(len(set(train_clean))))\n",
        "print(\"{} lexical tokens\".format(len(train_clean)))\n",
        "\n",
        "\"\"\"\n",
        "    What's our distribution look like?\n",
        "\"\"\"\n",
        "counts = Counter(train_clean)\n",
        "\n",
        "\n",
        "frequencies = [0]*8\n",
        "for w in counts:\n",
        "  if counts[w] >= 128:\n",
        "    frequencies[0] += 1\n",
        "  elif counts[w] >= 64:\n",
        "    frequencies[1] += 1\n",
        "  elif counts[w] >= 32:\n",
        "    frequencies[2] += 1\n",
        "  elif counts[w] >= 16:\n",
        "    frequencies[3] += 1\n",
        "  elif counts[w] >= 8:\n",
        "    frequencies[4] += 1\n",
        "  elif counts[w] >= 4:\n",
        "    frequencies[5] += 1\n",
        "  elif counts[w] >= 2:\n",
        "    frequencies[6] += 1\n",
        "  else:\n",
        "    frequencies[7] += 1\n",
        "\n",
        "\n",
        "# 2. Plot their distributions\n",
        "f,a = plt.subplots(1,1,figsize=(10,5))\n",
        "a.set(xlabel='Lexical types occuring more then n times', \n",
        "      ylabel='Number of lexical types')\n",
        "\n",
        "labels = [128, 64, 32, 16, 8, 4, 2, 1]\n",
        "_ = sns.barplot(labels, frequencies, ax=a, order=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kQ1O3NbRL0M",
        "colab_type": "text"
      },
      "source": [
        "This looks a lot better, because the bar of rare words that occur only once is smaller. There's probably more we can do, but let's keep moving forward.\n",
        "\n",
        "There will /always/ be rare words, and the model needs to know how to handle these one way or another. So, we're going to replace rare lexical types with an `unk`, indicating they are unknown. This will let our model keep writing when it bumps into a one-time made-up word like \"zombicorns.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKCex8fuRoLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.8\n",
        "\n",
        "counts_clean = Counter(train_clean)\n",
        "train_unk = [w if counts_clean[w] > 1 else \"unk\" for w in train_clean]\n",
        "valid_unk = [w if w in counts_clean and counts_clean[w] > 1 \\\n",
        "               else \"unk\" for w in valid_clean]\n",
        "\n",
        "# Let's plot these one last time\n",
        "counts = Counter(train_unk)\n",
        "\n",
        "frequencies = [0]*8\n",
        "for w in counts:\n",
        "  if counts[w] >= 128:\n",
        "    frequencies[0] += 1\n",
        "  elif counts[w] >= 64:\n",
        "    frequencies[1] += 1\n",
        "  elif counts[w] >= 32:\n",
        "    frequencies[2] += 1\n",
        "  elif counts[w] >= 16:\n",
        "    frequencies[3] += 1\n",
        "  elif counts[w] >= 8:\n",
        "    frequencies[4] += 1\n",
        "  elif counts[w] >= 4:\n",
        "    frequencies[5] += 1\n",
        "  elif counts[w] >= 2:\n",
        "    frequencies[6] += 1\n",
        "  else:\n",
        "    frequencies[7] += 1\n",
        "\n",
        "\n",
        "# 2. Plot their distributions\n",
        "f,a = plt.subplots(1,1,figsize=(10,5))\n",
        "a.set(xlabel='Lexical types occuring more then n times', \n",
        "      ylabel='Number of lexical types')\n",
        "\n",
        "labels = [128, 64, 32, 16, 8, 4, 2, 1]\n",
        "_ = sns.barplot(labels, frequencies, ax=a, order=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4jvogjOSkxZ",
        "colab_type": "text"
      },
      "source": [
        "Great! Now we've really cut down on our rare words. Let's take a look at what we're losing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPcAiPeRAW8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 1.9\n",
        "\n",
        "rare = [w for w in counts_clean if counts_clean[w] == 1]\n",
        "rare.sort()\n",
        "for line in wrap(\"   \".join([\"{:15s}\".format(w) for w in rare[-100:]]), width=70):\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXtWN3snTWpy",
        "colab_type": "text"
      },
      "source": [
        "`woahWOAHWOAH`, `wowzy`, `zrbajarb` ... we'll be fine without those words for today. So now we have our data, we're done with preprocessing, and we're ready to build a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ttbZ-mUIed",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Model Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAEt8AhuvNyt",
        "colab_type": "text"
      },
      "source": [
        "There are a couple key things that we need to do to build a model. First, we need to convert the sentences into lists or arrays of numbers. We want one word for every lexical type, so we’ll build a dictionary that assigns every word in our vocabulary a number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIcK-thVBcNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 2.1\n",
        "\n",
        "\"\"\"\n",
        "  Prepare our datasets by converting words to numbers\n",
        "\"\"\"\n",
        "# Create a mapping from words <-> numbers\n",
        "vocabulary = set(train_unk)\n",
        "word_to_num = {}\n",
        "num_to_word = {}\n",
        "for num, word in enumerate(vocabulary):\n",
        "  word_to_num[word] = num\n",
        "  num_to_word[num] = word\n",
        "\n",
        "# Convert our datasets into numbers\n",
        "import torch\n",
        "train = torch.LongTensor(len(train_unk))\n",
        "for i in range(len(train_unk)):\n",
        "  train[i] = word_to_num[train_unk[i]]\n",
        "\n",
        "valid = torch.LongTensor(len(valid_unk))\n",
        "for i in range(len(valid_unk)):\n",
        "  valid[i] = word_to_num[valid_unk[i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3mVbMMVV73q",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to convert our dataset into bite-sized pieces for the model.  Specifically, the `batch_size` is how many examples we look at during each step of training and the `seq_len` is how many words the model sees per example.  Here are some helpful functions that will do that for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FetTV331Vqsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 2.2\n",
        "\n",
        "# Parameters\n",
        "batch_size = 20   \n",
        "seq_len = 35        # CHANGEME\n",
        "\n",
        "# Tell Torch to use a GPU for computation\n",
        "device = torch.device(\"cuda\")\n",
        "# Setting the random seed decreases variability\n",
        "# Remove next three lines if running on your laptop\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# citation: https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "def get_batch(source, i, seq_len):\n",
        "    seq_len = min(seq_len, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "train = batchify(train, batch_size)\n",
        "valid = batchify(valid, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj6OxhsOWs13",
        "colab_type": "text"
      },
      "source": [
        "Now, let's actually build our AI! We’ll need two key parts, an embedding matrix and a recurrent neural network or RNN.\n",
        "\n",
        "An embedding matrix is a big list of vectors (which is basically a big table of numbers) where each row corresponds to a different word. These vector-rows capture how related two words are -- if two words are used in similar ways, then the numbers in their vectors should be similar. To start, every word will be assigned a vector with random numbers. \n",
        "\n",
        "A RNN is basically a model that incrementally builds a hidden representation by incorporating one new word at a time. The RNN’s output after reading the final word in part of a sentence is what we’ll use to predict the next word, and this will be a key part of training the model in Step 3. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAazfehEBgT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 2.3\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "        Define all the parameters of the model\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    # How tightly should we compress our language represenations?\n",
        "    self.embed_size = 300      # How big is our word vector   #ADVANCED_CHANGEME\n",
        "    self.hidden_size = 600     # How big is our hidden space  #ADVANCED_CHANGEME\n",
        "    \n",
        "    \"\"\" Converting words to Vectors \"\"\"\n",
        "    # A lookup table for translating a word into a vector\n",
        "    self.embedding = nn.Embedding(len(vocabulary), self.embed_size)\n",
        "    # Initialize our word vectors with a random uniform distribution\n",
        "    initrange = 0.1\n",
        "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    \n",
        "    \"\"\" An RNN (LSTM) with dropout \"\"\"\n",
        "    self.rnn = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size)\n",
        "    self.shrink = nn.Linear(self.hidden_size, self.embed_size)\n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "    \n",
        "    \"\"\" Predicting words from our model \"\"\"\n",
        "    # We convert our vector to a set of scores over words\n",
        "    self.decode = nn.Linear(self.embed_size, self.embedding.weight.size(0))\n",
        "    # We use the same matrix for this ``decoding'' that we used for ``encoding''\n",
        "    # https://arxiv.org/abs/1608.05859\n",
        "    self.decode.weight = self.embedding.weight\n",
        "    self.decode.bias.data.zero_()\n",
        "   \n",
        "  \n",
        "  def forward(self, input, hidden=None):\n",
        "    \"\"\"\n",
        "        Run the model\n",
        "    \"\"\"\n",
        "    # 1. Map words to vectors\n",
        "    embedded = self.embedding(input)\n",
        "    # 2. Process with an RNN\n",
        "    if hidden is not None:\n",
        "      output, hidden = self.rnn(embedded, hidden)\n",
        "    else:\n",
        "      output, hidden = self.rnn(embedded)\n",
        "    # 3. Apply dropout\n",
        "    output = F.relu(self.shrink(self.drop(output)))\n",
        "    # 4. Score the likelihood of every possible next word\n",
        "    decoded = self.decode(output)\n",
        "    return hidden, decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb4y0r7WXibS",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty3HvLO6wO1b",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to iterate over our dataset (specifically, those pieces called batches) and run backpropagation on each example to train the model’s weights. Over the span of one epoch of training this model, the network will loop over every batch of data -- reading it in, building representations, predicting the next word, and then updating its guesses. \n",
        "\n",
        "To start, we're going to train our model over 10 epochs, so this might take a couple minutes to run. \n",
        "\n",
        "We’ll print two numbers with each epoch, which are the model’s training and validation perplexities. As the model learns, it realizes there are fewer and fewer good choices for the next word. The perplexity is a measure of how well the model has narrowed down the choices. We can interpret perplexity as the average number of guesses the model makes before it predicts the right answer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPA-jER1YPN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 3.1\n",
        "\n",
        "import torch.nn.functional as F\n",
        "def training(model, data, targets, lr, hidden):\n",
        "  # Reset the model\n",
        "  model.zero_grad()\n",
        "\n",
        "  # Run the model to see its predictions and hidden states\n",
        "  hidden, prediction_vector = model(data, hidden)\n",
        "  prediction_vector = prediction_vector.view(seq_len * batch_size, -1)\n",
        "\n",
        "  # Compare the model's predictions at each timestep to the original data\n",
        "  loss = F.cross_entropy(prediction_vector, targets)\n",
        "  \n",
        "  # Compute gradients and perform back-propagation\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "  for p in model.parameters():\n",
        "      if p.grad is not None:\n",
        "        p.data.add_(-lr, p.grad.data)\n",
        "  \n",
        "  # Return the current model loss on this data item\n",
        "  return loss.item(), repackage_hidden(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEE6G8FhZ_px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 3.2\n",
        "\n",
        "def evaluation(model):\n",
        "  \"\"\"\n",
        "    This function performs almost all the same logic as the training function\n",
        "    but it does not perform backpropagation, because we don't want to learn\n",
        "    from this data, just check our performance.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  hidden = None\n",
        "  valid_loss = 0\n",
        "  for i in range(0, valid.size(0) - seq_len, seq_len):\n",
        "    data, targets = get_batch(valid, i, seq_len)\n",
        "    hidden, prediction_vector = model(data, hidden) \n",
        "    hidden = repackage_hidden(hidden)\n",
        "\n",
        "    prediction_vector = prediction_vector.view(-1, len(vocabulary))\n",
        "    loss = F.cross_entropy(prediction_vector, targets)\n",
        "    valid_loss += loss.item() \n",
        "  return valid_loss / (valid.size(0)/seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUM8SRBgBjBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 3.3\n",
        "\n",
        "# Create an instance of the model\n",
        "import numpy as np\n",
        "import time\n",
        "model = EncoderDecoder().float().to(device)\n",
        "prev_valid_loss = 1e100\n",
        "# This scales the size of each step of backpropagation\n",
        "learning_rate = 20\n",
        "# This value should match the batch_size used earlier for splitting up the data\n",
        "batch_size = 20\n",
        "\n",
        "num_epochs = 10                # CHANGEME\n",
        "timing = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # Set the model to training mode and iterate through the dataset\n",
        "  model.train()\n",
        "  hidden = None\n",
        "  train_loss = 0\n",
        "  start_time = time.time()\n",
        "  for i in range(0, train.size(0) - 1, seq_len):\n",
        "    # Get the next training batch\n",
        "    data, targets = get_batch(train, i, seq_len)\n",
        "    \n",
        "    # Run the model and perform backpropagation\n",
        "    loss, hidden = training(model, data, targets, learning_rate, hidden)\n",
        "    train_loss += loss\n",
        "\n",
        "  # Evaluate how well the model predicts unseen validation data\n",
        "  valid_loss = evaluation(model)\n",
        "\n",
        "  # Check if the model's ability to generalize has gotten worse.\n",
        "  # If so, slow the learning rate (shrink the step size)\n",
        "  if valid_loss > prev_valid_loss:\n",
        "    learning_rate /= 4.0\n",
        "\n",
        "  # Print the training and validation performance\n",
        "  train_loss /= (train.size(0)/seq_len)\n",
        "  finish_time = time.time()\n",
        "  print(\"Epoch {:2} took {:3.2f}s with train perplexity: {:7.2f}\"\\\n",
        "        \" and validation: {:7.2f}\".format(epoch, finish_time - start_time, \n",
        "                                          np.exp(train_loss), \n",
        "                                          np.exp(valid_loss)))\n",
        "  \n",
        "  prev_valid_loss = valid_loss\n",
        "\n",
        "total_time = (time.time() - timing)/60\n",
        "print(\"Completed {} epochs in {:5.3f} minutes\".format(num_epochs, total_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FN3WcghH3E7",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDZIYD6gczG0",
        "colab_type": "text"
      },
      "source": [
        "So far in Crash Course AI, we've looked at models that produce a \"best label\" or \"best prediction,\" but  here, there is no \"right\" answer. We're just building a generative model -- a model that can generate outputs.\n",
        "\n",
        "If we wrote stories by always having characters do the most obvious next thing, they’d be pretty boring. So we’re going to implement a basic sampler in our program, whicih will take a bunch of random paths instead of just choosing the highest-scoring next word every time. \n",
        "\n",
        "We can sort the results by the probability of the full sentences, and we can see which sentences are best overall. \n",
        "\n",
        "To start this generation, we need to give our model a word to begin every sentence. Let’s try “Good” for now, but you can try other things by changing the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkSJrjFhig8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 4.1\n",
        "\n",
        "# What do we want the model to start the sentence with?\n",
        "prefix = \"<s> Good\"      #CHANGEME\n",
        "\n",
        "# How many words do we want the model to produce?  \n",
        "words_to_generate = 50   #CHANGEME\n",
        "\n",
        "# We are only going to be looking at one example at a time\n",
        "batch_size = 1\n",
        "\n",
        "# Set the model to be in evaluation mode (no backprop!)\n",
        "model.eval()\n",
        "\n",
        "# Let's get lots of possible sentences\n",
        "argmax_sent = None\n",
        "argmax_prob = 0\n",
        "collection = []\n",
        "for item in range(100):\n",
        "  # Convert our sentence start into numbers\n",
        "  test = [word_to_num[word] if word in word_to_num else word_to_num[\"unk\"] \\\n",
        "          for word in prefix.split()]\n",
        "  probabilities = []\n",
        "  \n",
        "  # Run the model on the same initial input and it's own generations until\n",
        "  # we reach `word_to_generate`\n",
        "  for w in range(words_to_generate):\n",
        "    # Run the model\n",
        "    input = torch.from_numpy(np.array(test)).to(device)\n",
        "    _, output = model(input.view(-1,1))\n",
        "    \n",
        "    # Get the prediction for the last (next) word\n",
        "    last_pred = output[-1,:,:].squeeze()        \n",
        "    \n",
        "    # We're going to block generation of unk \n",
        "    last_pred[word_to_num[\"unk\"]] = -100   \n",
        "    \n",
        "    # Do we want to sample from this distrubtion?\n",
        "    if item > 0:\n",
        "      # A temperature makes the distribution peakier (if < 1) or flatter if > 1\n",
        "      last_pred /= 0.70   #ADVANCED_CHANGEME\n",
        "\n",
        "      # Turn this into a distribution\n",
        "      dist = torch.distributions.categorical.Categorical(logits=last_pred)\n",
        "      \n",
        "      # Sample\n",
        "      predicted_idx = dist.sample().item()\n",
        "      \n",
        "    else:\n",
        "      # If we aren't sampling, just take the most probable word\n",
        "      _, predicted_idx = last_pred.max(0)\n",
        "      predicted_idx = predicted_idx.item()\n",
        "\n",
        "    # Save the predicted word's probability\n",
        "    value = F.log_softmax(last_pred,-1)[predicted_idx].item()\n",
        "    \n",
        "    # Add this predicted word (index) to the list\n",
        "    test.append(predicted_idx)\n",
        "    # Save the probability for sorting later\n",
        "    probabilities.append(value)\n",
        "    \n",
        "  if item > 0:\n",
        "    # Add our sentence and its score to a list\n",
        "    generation = (np.exp(np.sum(probabilities)), \\\n",
        "                       \" \".join([num_to_word[w] for w in test]))\n",
        "    if generation not in collection:\n",
        "      collection.append(generation)\n",
        "  else:\n",
        "    argmax_sent = \" \".join([num_to_word[w] for w in test])\n",
        "    argmax_prob = np.exp(np.sum(probabilities))\n",
        "\n",
        "# Get the best model predictions\n",
        "collection.sort()\n",
        "collection.reverse()\n",
        "print(\"Argmax Generation:\")\n",
        "print(\"{:.2E}:  {}\\n\".format(argmax_prob,\"\\n\\t\\t\".join(wrap(argmax_sent))))\n",
        "print(\"\\nSampled Generations:\")\n",
        "for probability, sent in collection[:10]:\n",
        "  print(\"{:.2E}:  {}\\n\".format(probability, \"\\n\\t\\t\".join(wrap(sent))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiABIZxixgcb",
        "colab_type": "text"
      },
      "source": [
        "The sentence we get from taking the highest probability word each time (the Argmax) isn’t as interesting as the ones where we mixed it up a bit and took different branches (the other Sampled Generations). \n",
        "\n",
        "Building models that interact with people, and the world, is why natural language processing is so exciting, but it’ll take a lot more work to get our model to generate language as well as human John Green does.\n",
        "\n",
        "We’ve left a bunch of notes in the code for you to play with when making your own AI! You could train for longer, change the sentence prompt, or, if you’re feeling adventurous, replace the text data to speak in someone else’s voice. All you need is transcripts of their videos to get started.\n"
      ]
    }
  ]
}